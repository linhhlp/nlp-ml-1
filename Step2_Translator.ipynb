{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfd53c25",
   "metadata": {},
   "source": [
    "# 2. Translation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0675ea04",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f450f0f0",
   "metadata": {},
   "source": [
    "In this example, I will build LSTM single-layer neural networks for both the encoder and decoder. We need to take care of the final forward and backward states from a single layer to the decoder.\n",
    "\n",
    "```python\n",
    "# LSTM layer in Encoder\n",
    "lstm_layer = tf.keras.layers.LSTM( units, # dimensionality of the output space\n",
    "                                   return_sequences=True,  # Pass output sequence and state to Decoder \n",
    "                                   return_state=True,)\n",
    "```\n",
    "However, we can improve the accuracy by implementing BiLSTM or multi-layer LSTM/BiLSTM. Let's create a BiLSTM model with forward and backward layers:\n",
    "\n",
    "```python\n",
    "model = Sequential()\n",
    "forward_layer  = tf.keras.layers.LSTM(10, return_sequences=True)\n",
    "backward_layer = tf.keras.layers.LSTM(10, activation='relu', return_sequences=True,\n",
    "                                      go_backwards=True)\n",
    "model.add(tf.keras.layers.Bidirectional(forward_layer, backward_layer=backward_layer,\n",
    "                                        input_shape=(5, 10)))\n",
    "model.add(Dense(5))\n",
    "model.add(Activation('softmax'))\n",
    "```\n",
    "\n",
    "There is a tutorial to build [Encoder-Decoder Model using LSTM](https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence-prediction-keras/) and [compare LSTM with BiLSTM](https://machinelearningmastery.com/develop-bidirectional-lstm-sequence-classification-python-keras/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9482f801",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975b72ab",
   "metadata": {},
   "source": [
    "### Training Task\n",
    "\n",
    "There are two tasks during training:\n",
    "\n",
    "1. Input Task: given an input sequence (text) and extract useful information\n",
    "2. Output Task: we need to process the output properly to calculate the probability. So that we need Ground Truth Sequence as the given information and Final Token Sequence as a result which model should predict when giving the Ground Truth Sequence.\n",
    "\n",
    "```python\n",
    "dec_input = targ[ : , :-1 ]   # Ground Truth Sequence\n",
    "real = targ[ : , 1: ]         # Final Token Sequence\n",
    "pred = decoder(dec_input, decoder_initial_state)\n",
    "logits = pred.rnn_output\n",
    "loss = loss_function(real, logits)\n",
    "```\n",
    "\n",
    "#### Data cleaning\n",
    "\n",
    "Standardize Unicode letters and convert to ASCII to simplify the process. \n",
    "*unicodedata.normalize(form, unistr)* :This function returns the normal form for the Unicode string unistr. Valid values for form are ‘NFC’, ‘NFKC’, ‘NFD’, and ‘NFKD’.\n",
    "*unicodedata.category Mn* : Ignore NonSpacing Mark\n",
    "\n",
    "```python\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "```\n",
    "\n",
    "Below is a sample code how to deal with special letters \n",
    "\n",
    "\n",
    "```python\n",
    "w = unicode_to_ascii(w.lower().strip())\n",
    "w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w) # creating a space between a word and the punctuation following it\n",
    "w = re.sub(r'[\" \"]+', \" \", w)\n",
    "w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w) # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "w = w.strip()\n",
    "```\n",
    "\n",
    "#### Padding\n",
    "The length input/output is not given / fixed, such as translation, summarization of text. But the input of model is fixed when building neural networks. An extra symbol was filled into empty space called pad.\n",
    "```python\n",
    "tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "```\n",
    "\n",
    "#### Start and End of a Sentence\n",
    "The output is not required, but we need Machine returns something. So we use start-of-sequence \\<start> and end-of-sequence \\<end> tokens.\n",
    "```python\n",
    "w = '<start> ' + w + ' <end>'\n",
    "```\n",
    "\n",
    "#### Out of Vocabulary\n",
    "There are special words which do not exist in dictionary, we introduce Out-Of-Vocabulary (OOV) token.\n",
    "```python\n",
    "tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='<OOV>')\n",
    "```\n",
    "\n",
    "These extra symbols called new vocabulary or extended vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cbd420",
   "metadata": {},
   "source": [
    "### Attention\n",
    "\n",
    "There are two popular Attentions developed by Bahdanau (tfa.seq2seq.BahdanauAttention) and Luong (tfa.seq2seq.LuongAttention).\n",
    "Although the idea to use attention is easy to understand, implementation is complex. Fortunately, there is a helper in TensorFlow *AttentionWrapper* to add attention to the decoder cell.\n",
    "\n",
    "```python\n",
    "# Luong Attention\n",
    "attention_mechanism = tfa.seq2seq.LuongAttention(dec_units, memory, memory_sequence_length)\n",
    "rnn_cell = tfa.seq2seq.AttentionWrapper(tf.keras.layers.LSTMCell, \n",
    "                                  attention_mechanism, attention_layer_size=dec_units)\n",
    " \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f89622",
   "metadata": {},
   "source": [
    "### Decoding during Training\n",
    "\n",
    "During training, we have access to both the input and output sequences of a training pair. This means that we can use the output sequence's ground truth tokens as input for the decoder.\n",
    "\n",
    "The TrainingSampler object is initialized with the (embedded) ground truth sequences and the lengths of the ground truth sequences.\n",
    "\n",
    "```python\n",
    "sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "decoder = tfa.seq2seq.BasicDecoder(rnn_cell, sampler=sampler, output_layer=fc)\n",
    "```\n",
    "\n",
    "### Decoding during Inferencing\n",
    "\n",
    "When inferencing, there is no ground truth. Hence, we need to change TrainingSampler object to an inference helper. In this example, I show BasicDecoder from tf-addons which uses GreedyEmbeddingSampler. There is another helper [BeamSearchDecoder also from tf-addons](https://www.tensorflow.org/addons/tutorials/networks_seq2seq_nmt#use_tf-addons_beamsearchdecoder).\n",
    "\n",
    "```python\n",
    "greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler()\n",
    "decoder_instance = tfa.seq2seq.BasicDecoder(cell=decoder.rnn_cell, sampler=greedy_sampler, output_layer=decoder.fc)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abda1bdd",
   "metadata": {},
   "source": [
    "## Demo\n",
    "\n",
    "I will build a Translator from Vietnamese to English. The data was downloaded from http://www.manythings.org/anki/ and pre-processed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e10ee35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import time\n",
    "\n",
    "from NMTDataset import NMTDataset\n",
    "from models import Encoder, Decoder\n",
    "from functions import *\n",
    "\n",
    "def get_nmt():\n",
    "    path_to_file = \"./dict/vie-eng/vie.txt\"\n",
    "    return path_to_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bc3d5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Configuration parameters ############\n",
    "# DataSet#\n",
    "BUFFER_SIZE = 256000\n",
    "BATCH_SIZE = 64 # 32\n",
    "num_examples = 10000 # Let's limit the #training examples for faster training\n",
    "# Neural Network parameters #\n",
    "embedding_dim = 256\n",
    "units = 1024 # 128\n",
    "steps_per_epoch = num_examples//BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1a8e2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DataSet\n",
    "dataset_creator = NMTDataset('en-vie', get_nmt())\n",
    "train_dataset, val_dataset, inp_lang, targ_lang = dataset_creator.call(num_examples, BUFFER_SIZE, BATCH_SIZE)\n",
    "example_input_batch, example_target_batch = next(iter(train_dataset))\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "max_length_input = example_input_batch.shape[1]\n",
    "max_length_output = example_target_batch.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e738326c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test Encoder Stack\n",
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_h, sample_c = encoder(example_input_batch, sample_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b729285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test decoder stack\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE, max_length_input, max_length_output, 'luong')\n",
    "sample_x = tf.random.uniform((BATCH_SIZE, max_length_output))\n",
    "decoder.attention_mechanism.setup_memory(sample_output)\n",
    "initial_state = decoder.build_initial_state(BATCH_SIZE, [sample_h, sample_c], tf.float32)\n",
    "\n",
    "sample_decoder_outputs = decoder(sample_x, initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d137497",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss 0.8040 taken time  28.37 sec\n",
      "Epoch 2 Loss 0.6497 taken time  20.31 sec\n",
      "Epoch 3 Loss 0.5733 taken time  20.39 sec\n",
      "Epoch 4 Loss 0.5138 taken time  20.41 sec\n",
      "Epoch 5 Loss 0.4567 taken time  20.47 sec\n",
      "Epoch 6 Loss 0.4079 taken time  20.55 sec\n",
      "Epoch 7 Loss 0.3565 taken time  20.55 sec\n",
      "Epoch 8 Loss 0.3119 taken time  20.54 sec\n",
      "Epoch 9 Loss 0.2633 taken time  20.53 sec\n",
      "Epoch 10 Loss 0.2216 taken time  20.51 sec\n",
      "Epoch 11 Loss 0.1834 taken time  20.56 sec\n",
      "Epoch 12 Loss 0.1543 taken time  20.60 sec\n",
      "Epoch 13 Loss 0.1300 taken time  20.53 sec\n",
      "Epoch 14 Loss 0.1100 taken time  20.55 sec\n",
      "Epoch 15 Loss 0.0929 taken time  20.52 sec\n",
      "Epoch 16 Loss 0.0789 taken time  20.56 sec\n",
      "Epoch 17 Loss 0.0705 taken time  20.58 sec\n",
      "Epoch 18 Loss 0.0619 taken time  20.59 sec\n",
      "Epoch 19 Loss 0.0554 taken time  20.57 sec\n",
      "Epoch 20 Loss 0.0609 taken time  20.57 sec\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(train_dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden, BATCH_SIZE, encoder, decoder)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "    print('Epoch {} Loss {:.4f} taken time  {:.2f} sec'.format(epoch + 1, total_loss / steps_per_epoch, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3db150d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    result = evaluate_sentence(dataset_creator.preprocess_sentence(sentence), \n",
    "                               inp_lang, targ_lang, encoder, decoder, max_length_input, units) # \n",
    "    print(result)\n",
    "    result = targ_lang.sequences_to_texts(result) # Transform vertor numbers to words\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Translation: {}'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83e20659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  5  41 860   4   3]]\n",
      "Input: Tôi thích hoa.\n",
      "Translation: ['i like bread . <end>']\n"
     ]
    }
   ],
   "source": [
    "translate(u'Tôi thích hoa.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04a6fb0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  17   16   12   53 1818    4    3]]\n",
      "Input: Trời nắng.\n",
      "Translation: ['it s a very exciting . <end>']\n"
     ]
    }
   ],
   "source": [
    "translate(u'Trời nắng.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fe31a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  5 150   9   4   3]]\n",
      "Input: Anh yêu em.\n",
      "Translation: ['i love you . <end>']\n"
     ]
    }
   ],
   "source": [
    "translate(u'Anh yêu em.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b38b181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[516  42   4   3]]\n",
      "Input: Tiếp tục đi.\n",
      "Translation: ['hurry on . <end>']\n"
     ]
    }
   ],
   "source": [
    "translate(u'Tiếp tục đi.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
